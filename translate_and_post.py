#!/usr/bin/env python3
"""
ì¼ë³¸ ë“œë¡  ë‰´ìŠ¤ ìë™ ë²ˆì—­ ë° ì›Œë“œí”„ë ˆìŠ¤ ê²Œì‹œ ì‹œìŠ¤í…œ (ê°œì„ íŒ)
- ê¸°ëŠ¥: ì „ì²´ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘, ì´ë¯¸ì§€ ë³¸ë¬¸ ì‚½ì…, ê°•ì œ ì—…ë°ì´íŠ¸ ëª¨ë“œ ì§€ì›
"""

import os
import sys
import requests
import feedparser
from datetime import datetime, timedelta
from pathlib import Path
import json
import time
from urllib.parse import urlparse, urljoin
from googletrans import Translator
import html2text
from bs4 import BeautifulSoup  # HTML íŒŒì‹±ì„ ìœ„í•´ ì¶”ê°€

# ==========================================
# ì„¤ì • (Settings)
# ==========================================
WORDPRESS_URL = "https://grv.co.kr/wp"
WORDPRESS_USER = os.environ.get("WP_USER")
WORDPRESS_APP_PASSWORD = os.environ.get("WP_APP_PASSWORD")
DRONE_JP_RSS = "https://drone.jp/feed"
POSTED_ARTICLES_FILE = "posted_articles.json"

# [ìˆ˜ì • 2] ë®ì–´ì“°ê¸° ëª¨ë“œ (Trueë¡œ ì„¤ì •í•˜ë©´ ì´ë¯¸ ì˜¬ë¦° ê¸€ë„ ë‹¤ì‹œ ë²ˆì—­í•´ì„œ ìƒˆ ê¸€ë¡œ ë“±ë¡í•¨)
FORCE_UPDATE = True 

class NewsTranslator:
    def __init__(self):
        self.translator = Translator()
        self.wordpress_api = f"{WORDPRESS_URL}/wp-json/wp/v2"
        self.posted_articles = self.load_posted_articles()
        
    def load_posted_articles(self):
        """ì´ë¯¸ ê²Œì‹œëœ ê¸°ì‚¬ ëª©ë¡ ë¡œë“œ"""
        if Path(POSTED_ARTICLES_FILE).exists():
            with open(POSTED_ARTICLES_FILE, 'r') as f:
                try:
                    return json.load(f)
                except:
                    return []
        return []
        
    def save_posted_articles(self):
        """ê²Œì‹œëœ ê¸°ì‚¬ ëª©ë¡ ì €ì¥"""
        with open(POSTED_ARTICLES_FILE, 'w') as f:
            json.dump(self.posted_articles, f, indent=2)
        
    def fetch_rss_feed(self):
        """RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°"""
        print(f"ğŸ“¡ RSS í”¼ë“œ í™•ì¸ ì¤‘: {DRONE_JP_RSS}")
        feed = feedparser.parse(DRONE_JP_RSS)
        
        # 30ì¼ ì´ë‚´ ê¸°ì‚¬ê¹Œì§€ í—ˆìš© (ê¸°ê°„ ëŠ˜ë¦¼)
        limit_date = datetime.now() - timedelta(days=30)
        recent_articles = []
        
        print(f"ğŸ” ì´ {len(feed.entries)}ê°œì˜ í”¼ë“œ í•­ëª© ê²€ìƒ‰ ì‹œì‘...")

        for entry in feed.entries[:10]:  # ìµœì‹  10ê°œë§Œ ì§‘ì¤‘ ì²˜ë¦¬
            # [ìˆ˜ì • 2] FORCE_UPDATEê°€ êº¼ì ¸ìˆì„ ë•Œë§Œ ì¤‘ë³µ ì²´í¬
            if not FORCE_UPDATE and entry.link in self.posted_articles:
                print(f"  Pass (ì´ë¯¸ ê²Œì‹œë¨): {entry.title}")
                continue
                
            article_date = datetime(*entry.published_parsed[:6])
            if article_date > limit_date:
                recent_articles.append({
                    'title': entry.title,
                    'link': entry.link,
                    'date': article_date
                })
        
        print(f"âœ… ì²˜ë¦¬í•  ìƒˆ ê¸°ì‚¬: {len(recent_articles)}ê°œ")
        return recent_articles
        
    def fetch_full_content(self, url):
        """
        [ìˆ˜ì • 1] BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ê¸°ì‚¬ ë³¸ë¬¸ ì „ì²´ ìŠ¤í¬ë˜í•‘
        """
        try:
            print(f"ğŸ“„ ê¸°ì‚¬ ì›ë¬¸ ìŠ¤í¬ë˜í•‘ ì¤‘: {url}")
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # drone.jp ë° ì¼ë°˜ì ì¸ ì›Œë“œí”„ë ˆìŠ¤ ì‚¬ì´íŠ¸ì˜ ë³¸ë¬¸ ì˜ì—­ í´ë˜ìŠ¤ ì°¾ê¸°
            # ì‚¬ì´íŠ¸ë§ˆë‹¤ ë‹¤ë¥´ì§€ë§Œ ë³´í†µ entry-content, post-content ë“±ì„ ì‚¬ìš©í•¨
            content_div = soup.find('div', class_='entry-content')
            if not content_div:
                content_div = soup.find('div', class_='post-content')
            if not content_div:
                content_div = soup.find('article')
                
            if not content_div:
                print("âš ï¸ ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. RSS ìš”ì•½ë³¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return None

            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ê´‘ê³  ë“±)
            for tag in content_div(['script', 'style', 'iframe', 'noscript', 'form']):
                tag.decompose()
                
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ (HTML íƒœê·¸ë¥¼ Markdown ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ html2text ì‚¬ìš© ì¤€ë¹„)
            return str(content_div) # HTML ë¬¸ìì—´ ë°˜í™˜
            
        except Exception as e:
            print(f"âš ï¸ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None

    def translate_text(self, text):
        """ë²ˆì—­ í•¨ìˆ˜ (ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”)"""
        if not text: return ""
        
        try:
            # HTMLì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì´ë¯¸ì§€ íƒœê·¸ ë“±ì€ ìœ ì§€ë˜ì§€ ì•ŠìŒ -> í…ìŠ¤íŠ¸ ìœ„ì£¼ ë²ˆì—­)
            h = html2text.HTML2Text()
            h.ignore_links = False
            h.ignore_images = True # ì´ë¯¸ì§€ëŠ” ë³„ë„ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ í…ìŠ¤íŠ¸ë§Œ
            plain_text = h.handle(text)
            
            # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ ë²ˆì—­ (Google API ì œí•œ ëŒ€ë¹„)
            if len(plain_text) > 4000:
                chunks = [plain_text[i:i+4000] for i in range(0, len(plain_text), 4000)]
                translated_parts = []
                for chunk in chunks:
                    res = self.translator.translate(chunk, src='ja', dest='ko')
                    translated_parts.append(res.text)
                    time.sleep(1)
                return "\n".join(translated_parts)
            else:
                result = self.translator.translate(plain_text, src='ja', dest='ko')
                return result.text
        except Exception as e:
            print(f"âš ï¸ ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return text  # ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ë°˜í™˜

    def download_image(self, url):
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        if not url: return None
        try:
            print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ: {url}")
            res = requests.get(url, timeout=15)
            if res.status_code == 200:
                filename = os.path.basename(urlparse(url).path)
                if not filename: filename = "image.jpg"
                path = Path(f"/tmp/{filename}")
                with open(path, 'wb') as f:
                    f.write(res.content)
                return path
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì—ëŸ¬: {e}")
        return None

    def upload_media(self, image_path):
        """ì›Œë“œí”„ë ˆìŠ¤ ë¯¸ë””ì–´ ì—…ë¡œë“œ"""
        if not image_path: return None
        try:
            url = f"{self.wordpress_api}/media"
            headers = {
                'Content-Disposition': f'attachment; filename={image_path.name}',
                'Authorization': 'Basic '  # requests auth will handle this
            }
            with open(image_path, 'rb') as img:
                res = requests.post(
                    url,
                    auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                    headers=headers,
                    files={'file': img}
                )
                res.raise_for_status()
                return res.json() # ì „ì²´ JSON ë°˜í™˜ (source_url ë“± ì‚¬ìš© ìœ„í•´)
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def get_main_image_url(self, link):
        """Open Graph ë“±ì„ í†µí•´ ëŒ€í‘œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        try:
            res = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            soup = BeautifulSoup(res.text, 'lxml')
            
            # 1. Open Graph
            og_img = soup.find('meta', property='og:image')
            if og_img: return og_img['content']
            
            # 2. First Image in content
            img = soup.find('div', class_='entry-content').find('img')
            if img: return img['src']
            
        except:
            pass
        return None

    def post_to_wordpress(self, title, content, featured_media_id):
        """ì›Œë“œí”„ë ˆìŠ¤ í¬ìŠ¤íŠ¸ ìƒì„±"""
        post_data = {
            'title': title,
            'content': content,
            'status': 'publish',
            'featured_media': featured_media_id if featured_media_id else 0
        }
        
        try:
            res = requests.post(
                f"{self.wordpress_api}/posts",
                auth=(WORDPRESS_USER, WORDPRESS_APP_PASSWORD),
                json=post_data
            )
            res.raise_for_status()
            print(f"âœ¨ ê²Œì‹œ ì„±ê³µ! ë§í¬: {res.json()['link']}")
            return True
        except Exception as e:
            print(f"âŒ ê²Œì‹œ ì‹¤íŒ¨: {e}")
            if hasattr(e, 'response'): print(e.response.text)
            return False

    def process_article(self, article):
        print(f"\nğŸ“° ì²˜ë¦¬ ì‹œì‘: {article['title']}")
        
        # 1. ë³¸ë¬¸ ì „ì²´ ê°€ì ¸ì˜¤ê¸° [ìˆ˜ì • 1]
        raw_html = self.fetch_full_content(article['link'])
        if not raw_html:
            print("   ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í•´ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
            
        # 2. ë²ˆì—­ (ì œëª© ë° ë³¸ë¬¸)
        # ë³¸ë¬¸ì„ HTML ìƒíƒœì—ì„œ í…ìŠ¤íŠ¸ë§Œ ë½‘ì•„ ë²ˆì—­í•˜ê³  Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë¨
        title_ko = self.translate_text(article['title'])
        content_ko = self.translate_text(raw_html)
        
        # 3. ì´ë¯¸ì§€ ì²˜ë¦¬ [ìˆ˜ì • 3]
        img_url = self.get_main_image_url(article['link'])
        featured_id = 0
        uploaded_img_url = ""
        
        if img_url:
            local_img = self.download_image(img_url)
            media_info = self.upload_media(local_img)
            if media_info:
                featured_id = media_info['id']
                uploaded_img_url = media_info['source_url']
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                try: os.remove(local_img) 
                except: pass

        # 4. ë³¸ë¬¸ êµ¬ì„± (ì´ë¯¸ì§€ ì‚½ì… ë° ì›ë³¸ ë§í¬)
        # [ìˆ˜ì • 3] ì´ë¯¸ì§€ê°€ ë³¸ë¬¸ì— ë³´ì´ë„ë¡ ìµœìƒë‹¨ì— img íƒœê·¸ ì‚½ì…
        final_content = ""
        if uploaded_img_url:
            final_content += f'<img src="{uploaded_img_url}" alt="{title_ko}" style="width:100%; height:auto; margin-bottom: 20px;" /><br><br>'
        
        final_content += content_ko.replace("\n", "<br>") # ì¤„ë°”ê¿ˆ HTML ì²˜ë¦¬
        final_content += f"<br><br><hr><p>â„¹ï¸ <strong>ì›ë¬¸ ê¸°ì‚¬ ë³´ê¸°:</strong> <a href='{article['link']}' target='_blank'>{article['title']}</a></p>"
        
        # 5. ê²Œì‹œ
        if self.post_to_wordpress(title_ko, final_content, featured_id):
            if not FORCE_UPDATE: # ê°•ì œ ì—…ë°ì´íŠ¸ ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                self.posted_articles.append(article['link'])
                self.save_posted_articles()
            return True
        return False

    def run(self):
        print("ğŸš€ ë‰´ìŠ¤ ë²ˆì—­ ë´‡ ê°€ë™ ì‹œì‘")
        if not WORDPRESS_USER:
            print("âŒ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í•„ìš” (WP_USER, WP_APP_PASSWORD)")
            return

        articles = self.fetch_rss_feed()
        count = 0
        for article in articles:
            if self.process_article(article):
                count += 1
            time.sleep(3) # ì„œë²„ ë¶€í•˜ ë°©ì§€
            
        print(f"\nğŸ ì‘ì—… ì™„ë£Œ. ì´ {count}ê°œ ê²Œì‹œë¨.")

if __name__ == "__main__":
    bot = NewsTranslator()
    bot.run()
